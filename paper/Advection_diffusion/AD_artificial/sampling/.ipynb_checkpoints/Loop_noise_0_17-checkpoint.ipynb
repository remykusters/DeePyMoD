{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Advection-Diffusion equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this notebook we provide a simple example of the DeepMoD algorithm and apply it on the 2D advection-diffusion equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "# DeepMoD functions\n",
    "from deepymod import DeepMoD\n",
    "from deepymod.model.func_approx import NN\n",
    "from deepymod.model.library import Library2D_third\n",
    "from deepymod.model.constraint import LeastSquares\n",
    "from deepymod.model.sparse_estimators import Threshold,PDEFIND\n",
    "from deepymod.training import train\n",
    "from deepymod.training.sparsity_scheduler import TrainTestPeriodic\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Settings for reproducibility\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('../Diffusion_2D_space81.mat')\n",
    "data = np.real(data['Expression1']).reshape((81,81,81,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 81, 81, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datax = data[:,:,:,1]\n",
    "datay = data[:,:,:,2]\n",
    "datat = data[:,:,:,0]\n",
    "datau = data[:,:,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 81)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datau[:,:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 4\n",
    "tot_samples = number_of_samples*number_of_samples\n",
    "Utrain = np.empty([tot_samples,data.shape[2]])\n",
    "Xtrain = np.empty([tot_samples,data.shape[2],3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 81)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(datax.shape[2]):\n",
    "    idx = np.random.permutation(number_of_samples)\n",
    "    idy = np.random.permutation(number_of_samples)\n",
    "    Utrain[:,i] = np.array([datau[idx,k,i] for k in idy]).flatten()\n",
    "    Xtrain[:,i,1] = np.array([datax[idx,k,i] for k in idy]).flatten()\n",
    "    Xtrain[:,i,2] = np.array([datay[idx,k,i] for k in idy]).flatten()\n",
    "    Xtrain[:,i,0] = np.array([datat[idx,k,i] for k in idy]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 81)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Utrain.flatten()\n",
    "X = Xtrain.reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Add noise \n",
    "noise_level = 0.0\n",
    "y_noisy = y + noise_level * np.std(y) * np.random.randn(y.size, 1)\n",
    "\n",
    "# Randomize data \n",
    "\n",
    "idx = np.random.permutation(y.shape[0])\n",
    "X_train = torch.tensor(X[idx, :], dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_train = torch.tensor(y_noisy[idx, :], dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2425  MSE: 9.68e-04  Reg: 1.05e-09  L1: 1.45e+04 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-5d0ecb030bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlogdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'runs/test/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msparsity_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Github/DeePyMoD_sensor/src/deepymod/training/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, target, optimizer, sparsity_scheduler, split, exp_ID, log_dir, max_iterations, write_iterations, **convergence_kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# ====================== Logging =======================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_derivs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# calculating estimator coeffs but not setting mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             logger(iteration, \n\u001b[0m\u001b[1;32m     67\u001b[0m                    \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                    \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint_coeffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/DeePyMoD_sensor/src/deepymod/utils/logger.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iteration, loss, MSE, Reg, constraint_coeffs, unscaled_constraint_coeffs, estimator_coeffs, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0ml1_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstraint_coeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_tensorboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint_coeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munscaled_constraint_coeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_coeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/DeePyMoD_sensor/src/deepymod/utils/logger.py\u001b[0m in \u001b[0;36mupdate_tensorboard\u001b[0;34m(self, iteration, loss, loss_mse, loss_reg, loss_l1, constraint_coeff_vectors, unscaled_constraint_coeff_vectors, estimator_coeff_vectors, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Costs and coeff vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss/loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss/mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34mf'output_{idx}'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss/reg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34mf'output_{idx}'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss/l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34mf'output_{idx}'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_l1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_scalars\u001b[0;34m(self, main_tag, tag_scalar_dict, global_step, walltime)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0mcaffe2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mscalar_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFetchBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             fw.add_summary(scalar(main_tag, scalar_value),\n\u001b[0m\u001b[1;32m    393\u001b[0m                            global_step, walltime)\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_summary\u001b[0;34m(self, summary, global_step, walltime)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m     96\u001b[0m         \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_profile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_event\u001b[0;34m(self, event, step, walltime)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# Make sure step is converted from numpy or other formats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# since protobuf might not convert depending on version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Configure DeepMoD\n",
    "\n",
    "network = NN(3, [40, 40, 40, 40], 1)\n",
    "library = Library2D_third(poly_order=0) \n",
    "estimator = Threshold(0.05) \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=50, patience=200, delta=1e-5) \n",
    "constraint = LeastSquares() \n",
    "model = DeepMoD(network, library, estimator, constraint).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.99, 0.99), amsgrad=True, lr=2e-3) \n",
    "logdir='runs/test/'\n",
    "train(model, X_train, y_train, optimizer,sparsity_scheduler, log_dir=logdir, split=0.8, max_iterations=50000, delta=1e-6, patience=200) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dowmsampled shape: (17, 17, 41) Total number of data points: 11849\n",
      " 49975  MSE: 8.70e-06  Reg: 8.07e-06  L1: 1.64e+00 Algorithm converged. Writing model to disk.\n",
      "Dowmsampled shape: (17, 17, 21) Total number of data points: 6069\n",
      " 49975  MSE: 4.26e-06  Reg: 5.59e-06  L1: 1.43e+00 Algorithm converged. Writing model to disk.\n",
      "Dowmsampled shape: (17, 17, 11) Total number of data points: 3179\n",
      " 49975  MSE: 2.80e-06  Reg: 3.69e-06  L1: 1.47e+00 Algorithm converged. Writing model to disk.\n",
      "Dowmsampled shape: (17, 17, 7) Total number of data points: 2023\n",
      " 49975  MSE: 3.36e-06  Reg: 2.87e-06  L1: 1.41e+00 Algorithm converged. Writing model to disk.\n",
      "Dowmsampled shape: (17, 17, 6) Total number of data points: 1734\n",
      "  4700  MSE: 2.09e-04  Reg: 7.78e-06  L1: 1.00e+00 Algorithm converged. Writing model to disk.\n",
      "Dowmsampled shape: (17, 17, 5) Total number of data points: 1445\n",
      " 49975  MSE: 4.50e-05  Reg: 1.17e-05  L1: 1.71e+00 Algorithm converged. Writing model to disk.\n",
      "Dowmsampled shape: (17, 17, 4) Total number of data points: 1156\n",
      " 49975  MSE: 2.98e-05  Reg: 8.04e-06  L1: 1.42e+00 Algorithm converged. Writing model to disk.\n",
      "Dowmsampled shape: (17, 17, 3) Total number of data points: 867\n",
      " 49975  MSE: 5.90e-06  Reg: 2.42e-06  L1: 1.26e+00 Algorithm converged. Writing model to disk.\n"
     ]
    }
   ],
   "source": [
    "for i in time_range:\n",
    "    \n",
    "    # Downsample data and prepare data without noise:\n",
    "    down_data= np.take(np.take(np.take(data,np.arange(0,x_dim,5),axis=0),np.arange(0,y_dim,5),axis=1),np.arange(0,t_dim,i),axis=2)\n",
    "    print(\"Dowmsampled shape:\",down_data.shape, \"Total number of data points:\", np.product(down_data.shape))\n",
    "    index = len(np.arange(0,t_dim,i))    \n",
    "    width, width_2, steps = down_data.shape\n",
    "    x_arr, y_arr, t_arr = np.linspace(0,1,width), np.linspace(0,1,width_2), np.linspace(0,1,steps)\n",
    "    x_grid, y_grid, t_grid = np.meshgrid(x_arr, y_arr, t_arr, indexing='ij')\n",
    "    X, y = np.transpose((t_grid.flatten(), x_grid.flatten(), y_grid.flatten())), np.float32(down_data.reshape((down_data.size, 1)))\n",
    "    \n",
    "    \n",
    "    # Add noise \n",
    "    noise_level = 0.0\n",
    "    y_noisy = y + noise_level * np.std(y) * np.random.randn(y.size, 1)\n",
    "\n",
    "    # Randomize data \n",
    "\n",
    "    idx = np.random.permutation(y.shape[0])\n",
    "    X_train = torch.tensor(X[idx, :], dtype=torch.float32, requires_grad=True).to(device)\n",
    "    y_train = torch.tensor(y_noisy[idx, :], dtype=torch.float32).to(device)\n",
    "\n",
    "    # Configure DeepMoD\n",
    "\n",
    "    network = NN(3, [40, 40, 40, 40], 1)\n",
    "    library = Library2D_third(poly_order=0) \n",
    "    estimator = Threshold(0.05) \n",
    "    sparsity_scheduler = TrainTestPeriodic(periodicity=50, patience=200, delta=1e-5) \n",
    "    constraint = LeastSquares() \n",
    "    model = DeepMoD(network, library, estimator, constraint).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), betas=(0.99, 0.99), amsgrad=True, lr=2e-3) \n",
    "    logdir='final_runs/no_noise_x17/'+str(index)+'/'\n",
    "    train(model, X_train, y_train, optimizer,sparsity_scheduler, log_dir=logdir, split=0.8, max_iterations=50000, delta=1e-6, patience=200) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
